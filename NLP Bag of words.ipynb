{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP Bag of words.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN198mGcu35mymdGgKnmRXz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvQFWmMdRxZA","executionInfo":{"status":"ok","timestamp":1633975601680,"user_tz":-300,"elapsed":8,"user":{"displayName":"Talha Rauf","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjc6SCa91C29MQ8I0hgZQ2ZfF3L1NcvYRz5ja9M=s64","userId":"06626148515745320759"}},"outputId":"f3544966-735f-4ecc-9346-c05a3392963d"},"source":["  import nltk\n","  nltk.download('punkt')\n","  nltk.download('stopwords')\n","  from nltk.corpus import wordnet as wn\n","  import sklearn\n","  import os\n","  import re\n","  import pandas as pd\n","  import math\n","  import collections as cl\n","  import scipy as sp\n","  import numpy as np\n"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"HgRaQ4TBSB5T","executionInfo":{"status":"ok","timestamp":1633975605277,"user_tz":-300,"elapsed":918,"user":{"displayName":"Talha Rauf","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjc6SCa91C29MQ8I0hgZQ2ZfF3L1NcvYRz5ja9M=s64","userId":"06626148515745320759"}}},"source":["from nltk.corpus.reader import wordnet\n","# Uploading the file\n","my_file = open(\"input_script.txt\", \"rt\") # open lorem.txt for reading text\n","contents = my_file.read()         # read the entire file to string\n","\n","\n","#Cleaning the Text\n","\n","# Below the code for removal of meta-deta\n","import re \n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer  # For Steming the word\n","from nltk.stem import WordNetLemmatizer # It's an Word Limitizer\n","\n","# Now Create Objects for cleaning Purposes\n","\n","ps = PorterStemmer()\n","wordNet  =WordNetLemmatizer()\n","sentences = nltk.sent_tokenize(contents)\n","corpus = []\n","for i in range(len(sentences)):\n","  review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n","  review = review.lower()\n","  review = review.split()\n","  review = [word for word in review if word not in stopwords.words('english')]\n","  #review = [wordnet.lemmatize(word) for word in review if not word in set (stopwords.words('english'))]\n","  review = ' '.join(review)\n","  corpus.append(review)\n","  review\n","\n"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1LoiDfe8SXt6","executionInfo":{"status":"ok","timestamp":1633977395470,"user_tz":-300,"elapsed":360,"user":{"displayName":"Talha Rauf","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjc6SCa91C29MQ8I0hgZQ2ZfF3L1NcvYRz5ja9M=s64","userId":"06626148515745320759"}},"outputId":"7c7565de-bf96-4eba-c163-31e3955ae2ed"},"source":["#Creating the Bag of Words\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","cv  = CountVectorizer()\n","X = cv.fit_transform(corpus).toarray()\n","X"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0]])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"rIhb33bfi5Mm"},"source":[""],"execution_count":null,"outputs":[]}]}