{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N Grams and Curse of Dimensionality.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvQFWmMdRxZA",
        "outputId": "34ccd1df-3360-4cfd-bf88-f6b381d663ab"
      },
      "source": [
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')\n",
        "  from nltk.corpus import wordnet as wn\n",
        "  import sklearn\n",
        "  import os\n",
        "  import re\n",
        "  import pandas as pd\n",
        "  import math\n",
        "  import collections as cl\n",
        "  import scipy as sp\n",
        "  import numpy as np\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgRaQ4TBSB5T"
      },
      "source": [
        "from nltk.corpus.reader import wordnet\n",
        "# Uploading the file\n",
        "my_file = open(\"input_script.txt\", \"rt\") # open lorem.txt for reading text\n",
        "contents = my_file.read()         # read the entire file to string\n",
        "\n",
        "\n",
        "#Cleaning the Text\n",
        "\n",
        "# Below the code for removal of meta-deta\n",
        "import re \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer  # For Steming the word\n",
        "from nltk.stem import WordNetLemmatizer # It's an Word Limitizer\n",
        "\n",
        "# Now Create Objects for cleaning Purposes\n",
        "\n",
        "ps = PorterStemmer()\n",
        "wordNet  =WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(contents)\n",
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [word for word in review if word not in stopwords.words('english')]\n",
        "  #review = [wordnet.lemmatize(word) for word in review if not word in set (stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)\n",
        "  review\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LoiDfe8SXt6",
        "outputId": "a2f84b2b-e3a1-4417-8fde-217cfa87791b"
      },
      "source": [
        "#Creating the Bag of Words\n",
        "#Count Vectorization\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv  = CountVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "X"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIhb33bfi5Mm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48fc87c-4929-4f92-fd1e-5b2d348eb03f"
      },
      "source": [
        "# N-Grams, Implementation of Bi Grams\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv  = CountVectorizer(ngram_range = (2,2))\n",
        "corpus = [\"This is a first sentence\",\n",
        "          \"This is an another sentence\",\n",
        "          \"that the third sentence\"]\n",
        "\n",
        "X = cv.fit_transform(corpus)\n",
        "print(X.shape)\n",
        "print(X.toarray())\n",
        "\n",
        "df = pd.DataFrame(X.toarray(), columns = cv.get_feature_names())\n",
        "print(df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 9)\n",
            "[[0 0 1 0 1 0 0 0 1]\n",
            " [1 1 0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 1 1 0]]\n",
            "   an another  another sentence  ...  third sentence  this is\n",
            "0           0                 0  ...               0        1\n",
            "1           1                 1  ...               0        1\n",
            "2           0                 0  ...               1        0\n",
            "\n",
            "[3 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X-dixP4UGdd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}